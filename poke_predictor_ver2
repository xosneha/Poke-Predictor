"""
Pokemon name character-level RNN model. RNN inspired by Andrej Karpathy 
Written by Sneha Shinde
"""
import numpy as np
import pandas as pd
import requests

from collections import deque
from json import dump, load

API_URL = 'https://pokeapi.co/api/v2/'
FIRST_RUN=False
POKES_WITH_DASH = frozenset({"type-null", "jangmo-o", "hakamo-o", "kommo-o", "tapu-koko", "tapu-lele", "tapu-bulu", "tapu-fini"})
FNAME = 'cached_pokemon.json'
# hyperparameters
HIDDEN_SIZE = 100 # size of hidden layer of neurons
SEQ_LENGTH = 25 # number of steps to unroll the RNN for
LEARNING_RATE = 1e-1
# Run this ONLY ONCE
######


if FIRST_RUN:
    res = requests.get(f'{API_URL}/pokemon').json()
    pokemon = []
    while True:
        pokemon += [poke['name'] for poke in res['results']]
        if not res['next']:
            break
        res = requests.get(res['next']).json()
    with open(FNAME, 'w') as fp:
        dump(pokemon, fp)

#######
# Use this after
else:
    with open(FNAME, 'r') as fp:
        pokemon = load(fp)

data = ''
for poke in pokemon:
    if '-' in poke and poke in POKES_WITH_DASH:
        data += poke + '\n'
    elif '-' in poke:
        poke, *_ = poke.split('-') 
        if poke not in data:
            data += poke + '\n'
    else:
        data += poke + '\n'

chars = list(set(data)) 

data_size, vocab_size = len(data), len(chars)
print(f'data has {data_size} characters, {vocab_size} unique.')
char_to_ix = { ch: i for i, ch in enumerate(chars) }
ix_to_char = { i: ch for i, ch in enumerate(chars) }

# model parameters
Wxh = np.random.randn(HIDDEN_SIZE, vocab_size) * 0.01 # input to hidden
Whh = np.random.randn(HIDDEN_SIZE, HIDDEN_SIZE) * 0.01 # hidden to hidden
Why = np.random.randn(vocab_size, HIDDEN_SIZE) * 0.01 # hidden to output
bh = np.zeros((HIDDEN_SIZE, 1)) # hidden bias
by = np.zeros((vocab_size, 1)) # output bias

def lossFun(inputs, targets, hprev):
  """
  inputs,targets are both list of integers.
  hprev is Hx1 array of initial hidden state
  returns the loss, gradients on model parameters, and last hidden state
  """
  xs, hs, ys, ps = {}, {}, {}, {}
  hs[-1] = np.copy(hprev)
  loss = 0
  # forward pass
  for t in range(len(inputs)):
    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation
    xs[t][inputs[t]] = 1
    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state
    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars
    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars
    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)
  # backward pass: compute gradients going backwards
  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)
  dbh, dby = np.zeros_like(bh), np.zeros_like(by)
  dhnext = np.zeros_like(hs[0])
  for t in reversed(range(len(inputs))):
    dy = np.copy(ps[t])
    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here
    dWhy += np.dot(dy, hs[t].T)
    dby += dy
    dh = np.dot(Why.T, dy) + dhnext # backprop into h
    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity
    dbh += dhraw
    dWxh += np.dot(dhraw, xs[t].T)
    dWhh += np.dot(dhraw, hs[t-1].T)
    dhnext = np.dot(Whh.T, dhraw)
  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:
    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients
  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]

def sample(h, seed_ix, n):
  """ 
  sample a sequence of integers from the model 
  h is memory state, seed_ix is seed letter for first time step
  """
  x = np.zeros((vocab_size, 1))
  x[seed_ix] = 1
  ixes = []
  for t in range(n):
    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)
    y = np.dot(Why, h) + by
    p = np.exp(y) / np.sum(np.exp(y))
    ix = np.random.choice(list(range(vocab_size)), p=p.ravel())
    x = np.zeros((vocab_size, 1))
    x[ix] = 1
    ixes.append(ix)
  return ixes

n = p = 0
mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)
mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad
smooth_loss = -np.log(1.0 / vocab_size) * SEQ_LENGTH # loss at iteration 0

NUM_HISTORY = 10 # how many past losses we want to check to see if we should stop
LOSS_THRESHOLD = 1e-7 # minimum amount our cur loss can be better than the avg of the history
PRINT_ITER_COUNT = 5000 # when should we start printing poke names and loss
past_losses = deque(maxlen=NUM_HISTORY)

while True:
  # prepare inputs (we're sweeping from left to right in steps seq_length long)
  if p + SEQ_LENGTH + 1 >= len(data) or n == 0: 
    hprev = np.zeros((HIDDEN_SIZE,1)) # reset RNN memory
    p = 0 # go from start of data
  inputs = [char_to_ix[ch] for ch in data[p:p+SEQ_LENGTH]]
  targets = [char_to_ix[ch] for ch in data[p+1:p+SEQ_LENGTH+1]]

  # sample from the model now and then
  if n % 100 == 0 and n >= PRINT_ITER_COUNT:
    sample_ix = sample(hprev, inputs[0], 200)
    txt = ''.join(ix_to_char[ix] for ix in sample_ix)
    print(f'----\n {txt} \n----')

  # forward seq_length characters through the net and fetch gradient
  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)
  smooth_loss = smooth_loss * 0.999 + loss * 0.001
  if n % 100 == 0 and n >= PRINT_ITER_COUNT: print(f'iter {n}, loss: {smooth_loss}') # print progress
  if len(past_losses) == NUM_HISTORY and abs(sum(past_losses)/len(past_losses) - smooth_loss) < LOSS_THRESHOLD:
      print(past_losses)
      print(smooth_loss)
      print(sum(past_losses)/len(past_losses))
      break 
  # perform parameter update with Adagrad
  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], 
                                [dWxh, dWhh, dWhy, dbh, dby], 
                                [mWxh, mWhh, mWhy, mbh, mby]):
    mem += dparam * dparam
    param += -LEARNING_RATE * dparam / np.sqrt(mem + 1e-8) # adagrad update

  p += SEQ_LENGTH # move data pointer
  n += 1 # iteration counter

  past_losses.append(smooth_loss)